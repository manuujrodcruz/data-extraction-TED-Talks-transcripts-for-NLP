"""
M√≥dulo principal que importa y configura todas las funcionalidades
"""

# Importar todas las librer√≠as b√°sicas
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n inicial
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 50)

# Constante para archivo de datos
DEFAULT_DATA_FILE = "ted_talks_en.csv"

# Importar tracker de progreso
try:
    from .progress_tracker import ProgressTracker, real_time_feedback
    print("OK - Sistema de progreso en tiempo real cargado")
except ImportError as e:
    print(f"ERROR importando progress_tracker: {e}")

print("Cargando m√≥dulos del proyecto TED Talks...")

# Importar m√≥dulos locales
try:
    from .environment_setup import (
        setup_environment, 
        check_device,
    )
    print("M√≥dulo de configuraci√≥n del ambiente cargado")
except ImportError as e:
    print(f"Error importando environment_setup: {e}")

try:
    from .data_cleaner import (
        clean_dataset_professional,
        validate_data_quality
    )
    print("M√≥dulo de limpieza de datos cargado")
except ImportError as e:
    print(f"Error importando data_cleaner: {e}")

try:
    from .nlp_processor import (
        load_nlp_models,
        process_text_features,
        create_word_frequency_analysis
    )
    print("M√≥dulo de procesamiento NLP cargado")
except ImportError as e:
    print(f"Error importando nlp_processor: {e}")

try:
    from .visualizer import (
        create_data_overview_plots,
        create_correlation_heatmap,
        create_sentiment_analysis_plots,
        create_text_features_plots,
        create_entity_analysis_plots,
        create_wordcloud,
        create_interactive_plots,
        print_summary_statistics
    )
    print("M√≥dulo de visualizaci√≥n cargado")
except ImportError as e:
    print(f"Error importando visualizer: {e}")

try:
    from .ml_models import (
        TedTalkClassifier,
        create_ml_pipeline
    )
    print("M√≥dulo de machine learning cargado")
except ImportError as e:
    print(f"Error importando ml_models: {e}")


class TedTalkAnalyzer:
    """
    Clase principal que orquesta todo el an√°lisis de TED Talks
    """
    
    def __init__(self):
        self.data = None
        self.df_clean = None
        self.df_processed = None
        self.nlp_models = None
        self.ml_classifier = None
        self.results = {}
        
    def load_data(self, file_path=DEFAULT_DATA_FILE):
        """Carga el dataset inicial"""
        print(f"\n=== CARGANDO DATASET: {file_path} ===")
        
        try:
            self.data = pd.read_csv(file_path)
            print(f"‚úì Dataset cargado: {self.data.shape[0]} filas x {self.data.shape[1]} columnas")
            
            # Mostrar informaci√≥n b√°sica
            print("\nColumnas disponibles:")
            for i, col in enumerate(self.data.columns, 1):
                print(f"{i:2d}. {col}")
            
            self.results['data_loaded'] = True
            return self.data
            
        except Exception as e:
            print(f"‚úó Error cargando dataset: {e}")
            self.results['data_loaded'] = False
            return None
    
    def setup_environment(self):
        """Configura el ambiente y modelos necesarios"""
        print("\n=== CONFIGURANDO AMBIENTE ===")
        
        try:
            # Configurar ambiente
            setup_environment()
            
            # Verificar dispositivo
            device = check_device()
            
            # Cargar modelos NLP
            self.nlp_models = load_nlp_models()
            
            self.results['environment_setup'] = True
            self.results['device'] = device
            
            print("‚úì Ambiente configurado correctamente")
            
        except Exception as e:
            print(f"‚úó Error configurando ambiente: {e}")
            self.results['environment_setup'] = False
    
    def clean_data(self):
        """Limpia y prepara los datos"""
        if self.data is None:
            print("Primero debes cargar los datos")
            return None
        
        print("\n=== LIMPIANDO DATOS ===")
        
        try:
            self.df_clean, cleaning_log = clean_dataset_professional(self.data)
            
            # Validar calidad
            quality_results = validate_data_quality(self.df_clean)
            
            self.results['data_cleaning'] = {
                'original_shape': self.data.shape,
                'clean_shape': self.df_clean.shape,
                'cleaning_log': cleaning_log,
                'quality_results': quality_results
            }
            
            print("Datos limpiados correctamente")
            return self.df_clean
            
        except Exception as e:
            print(f"‚úó Error limpiando datos: {e}")
            return None
    
    def process_nlp_features(self, text_column='transcript_clean'):
        """Procesa caracter√≠sticas de NLP"""
        if self.df_clean is None:
            print("Primero debes limpiar los datos")
            return None
        
        print("\n=== PROCESANDO CARACTER√çSTICAS NLP ===")
        
        try:
            # Procesar caracter√≠sticas de texto
            self.df_processed = process_text_features(
                self.df_clean, 
                text_column=text_column, 
                nlp_models=self.nlp_models
            )
            
            # An√°lisis de frecuencia de palabras
            word_frequencies = create_word_frequency_analysis(
                self.df_processed, 
                text_column, 
                stop_words=self.nlp_models['stop_words'] if self.nlp_models else None
            )
            
            self.results['nlp_processing'] = {
                'text_column': text_column,
                'word_frequencies': word_frequencies,
                'features_added': [col for col in self.df_processed.columns if 
                                 col.startswith(('sentiment_', 'text_', 'person_', 'org_', 'gpe_'))]
            }
            
            print("‚úì Caracter√≠sticas NLP procesadas correctamente")
            return self.df_processed
            
        except Exception as e:
            print(f"‚úó Error procesando NLP: {e}")
            return None
    
    def create_visualizations(self):
        """Crea todas las visualizaciones"""
        if self.df_clean is None:
            print("Primero debes procesar los datos")
            return
        
        print("\n=== CREANDO VISUALIZACIONES ===")
        
        try:
            # Resumen de estad√≠sticas
            print_summary_statistics(self.df_processed)
            
            # Visualizaciones principales
            create_data_overview_plots(self.df_processed)
            
            # Matriz de correlaci√≥n
            numeric_columns = self.df_processed.select_dtypes(include=[np.number]).columns.tolist()
            create_correlation_heatmap(self.df_processed, numeric_columns)

            # An√°lisis de sentimientos
            create_sentiment_analysis_plots(self.df_processed)

            # Caracter√≠sticas textuales
            create_text_features_plots(self.df_processed)

            # An√°lisis de entidades
            create_entity_analysis_plots(self.df_processed)
            
            # Nube de palabras
            if 'transcript_clean' in self.df_processed.columns:
                create_wordcloud(self.df_processed['transcript_clean'],
                                 title="Nube de Palabras - Transcripciones TED Talks")
            
            # Gr√°ficos interactivos
            create_interactive_plots(self.df_processed)

            self.results['visualizations'] = True
            print("‚úì Visualizaciones creadas correctamente")
            
        except Exception as e:
            print(f"‚úó Error creando visualizaciones: {e}")
            self.results['visualizations'] = False
    
    def train_models(self, text_column='transcript_clean', target_column='popularity_numeric'):
        """Entrena modelos de machine learning"""
        if self.df_clean is None:
            print("Primero debes procesar los datos")
            return None
        
        print("\n=== ENTRENANDO MODELOS DE MACHINE LEARNING ===")
        
        try:
            # Crear y ejecutar pipeline de ML
            self.ml_classifier, ml_results = create_ml_pipeline(
                self.df_processed,
                text_column=text_column,
                target_column=target_column
            )
            
            self.results['machine_learning'] = {
                'models_trained': list(self.ml_classifier.models.keys()),
                'evaluation_results': ml_results,
                'best_model': self.ml_classifier.get_best_model()
            }
            
            print("‚úì Modelos entrenados correctamente")
            return self.ml_classifier
            
        except Exception as e:
            print(f"‚úó Error entrenando modelos: {e}")
            return None
    
    def run_complete_analysis(self, file_path=DEFAULT_DATA_FILE, text_column='transcript_clean'):
        """Ejecuta el an√°lisis completo"""
        print("INICIANDO ANALISIS COMPLETO DE TED TALKS")
        print("=" * 60)
        
        # Paso 1: Configurar ambiente
        self.setup_environment()
        
        # Paso 2: Cargar datos
        self.load_data(file_path)
        
        # Paso 3: Limpiar datos
        self.clean_data()
        
        # Paso 4: Procesar NLP
        self.process_nlp_features(text_column)
        
        # Paso 5: Crear visualizaciones
        self.create_visualizations()
        
        # Paso 6: Entrenar modelos
        self.train_models(text_column)
        
        # Resumen final
        self.print_final_summary()
        
        print("\nANALISIS COMPLETO FINALIZADO")
        return self.results
    
    def print_final_summary(self):
        """Imprime un resumen final del an√°lisis"""
        print("\n" + "=" * 60)
        print("RESUMEN FINAL DEL AN√ÅLISIS")
        print("=" * 60)
        
        if self.results.get('data_loaded'):
            original_shape = self.results['data_cleaning']['original_shape']
            clean_shape = self.results['data_cleaning']['clean_shape']
            print(f"Datos procesados: {original_shape[0]} ‚Üí {clean_shape[0]} filas")
            print(f"Calidad de datos: {self.results['data_cleaning']['quality_results']['quality_score']:.2f}/10")
        
        if self.results.get('nlp_processing'):
            features_count = len(self.results['nlp_processing']['features_added'])
            print(f"Caracter√≠sticas NLP creadas: {features_count}")
        
        if self.results.get('machine_learning'):
            models_count = len(self.results['machine_learning']['models_trained'])
            best_model_info = self.results['machine_learning']['best_model']
            print(f"Modelos entrenados: {models_count}")
            if best_model_info[0]:
                print(f"üèÜ Mejor modelo: {best_model_info[0]} (F1: {best_model_info[2]:.4f})")
        
        # Estado general
        steps_completed = sum([
            self.results.get('environment_setup', False),
            self.results.get('data_loaded', False),
            'data_cleaning' in self.results,
            'nlp_processing' in self.results,
            self.results.get('visualizations', False),
            'machine_learning' in self.results
        ])
        
        print(f"Pasos completados: {steps_completed}/6")
        
        if steps_completed == 6:
            print("¬°An√°lisis 100% completo!")
        else:
            print("Algunos pasos no se completaron correctamente")


def quick_start(file_path=DEFAULT_DATA_FILE):
    """
    Funci√≥n de inicio r√°pido para ejecutar todo el an√°lisis
    """
    analyzer = TedTalkAnalyzer()
    results = analyzer.run_complete_analysis(file_path)
    return analyzer, results


def quick_test():
    """
    Funci√≥n de prueba r√°pida para verificar que todo funciona
    """
    from datetime import datetime
    
    tracker = ProgressTracker(total_steps=4, description="Prueba r√°pida")
    tracker.start("Iniciando verificaci√≥n r√°pida del sistema")
    
    try:
        # Paso 1: Verificar imports b√°sicos
        tracker.step("Verificando imports b√°sicos")
        import pandas as pd
        import numpy as np
        import sklearn
        real_time_feedback("Librer√≠as b√°sicas: OK")
        
        # Paso 2: Verificar datos
        tracker.step("Verificando acceso a datos")
        try:
            df = pd.read_csv(DEFAULT_DATA_FILE)
            real_time_feedback(f"Dataset cargado: {df.shape[0]:,} filas")
        except FileNotFoundError:
            real_time_feedback(" Dataset no encontrado - usando datos sint√©ticos")
            df = pd.DataFrame({'test': [1,2,3]})
        
        # Paso 3: Verificar m√≥dulos del proyecto  
        tracker.step("Verificando m√≥dulos del proyecto")
        functions_available = [
            'setup_environment' in globals(),
            'clean_dataset_professional' in globals(),
            'process_text_features' in globals(),
            'create_ml_pipeline' in globals()
        ]
        available_count = sum(functions_available)
        real_time_feedback(f"M√≥dulos disponibles: {available_count}/4")
        
        # Paso 4: Verificar configuraci√≥n
        tracker.step("Verificando configuraci√≥n del ambiente")
        try:
            from textblob import TextBlob
            blob = TextBlob("test")
            real_time_feedback("TextBlob: OK")
        except:
            real_time_feedback("TextBlob: No disponible")
            
        tracker.finish("Verificaci√≥n completada")
        
        print("\n RESULTADO DE LA PRUEBA:")
        print("=" * 40)
        print(f" Librer√≠as b√°sicas: Funcionando")
        print(f" Acceso a datos: {'OK' if 'df' in locals() else 'Limitado'}")
        print(f" M√≥dulos del proyecto: {available_count}/4 disponibles")
        print(f" Verificaci√≥n completada: {datetime.now().strftime('%H:%M:%S')}")
        print("\n Puedes proceder con el an√°lisis completo")
        
        return True
        
    except Exception as e:
        tracker.finish(f"Error en verificaci√≥n: {e}")
        print(f"\n ERROR: {e}")
        return False