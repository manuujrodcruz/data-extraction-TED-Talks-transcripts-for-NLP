# ğŸ“Š Proyecto de AnÃ¡lisis de TED Talks - Arquitectura Modular

## ğŸ¯ DescripciÃ³n

Este proyecto implementa un anÃ¡lisis completo de TED Talks utilizando tÃ©cnicas de NLP y Machine Learning, organizado en mÃ³dulos Python reutilizables para mÃ¡xima flexibilidad y mantenibilidad.

## ğŸ“ Estructura del Proyecto

```
data-extraction-TED-Talks-transcripts-for-NLP/
â”œâ”€â”€ ted_talks_en.csv                 # Dataset principal
â”œâ”€â”€ versionmanuel.ipynb             # Notebook con ejemplos de uso
â”œâ”€â”€ modules/                        # MÃ³dulos Python organizados
â”‚   â”œâ”€â”€ __init__.py                # MÃ³dulo principal e importaciones
â”‚   â”œâ”€â”€ environment_setup.py       # ConfiguraciÃ³n del ambiente
â”‚   â”œâ”€â”€ data_cleaner.py            # Limpieza profesional de datos
â”‚   â”œâ”€â”€ nlp_processor.py           # Procesamiento de lenguaje natural
â”‚   â”œâ”€â”€ visualizer.py              # Visualizaciones y grÃ¡ficos
â”‚   â””â”€â”€ ml_models.py               # Modelos de machine learning
â””â”€â”€ README.md                      # Este archivo
```

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: AnÃ¡lisis Completo Automatizado
```python
from modules import quick_start

# Ejecuta todo el pipeline automÃ¡ticamente
analyzer, results = quick_start('ted_talks_en.csv')
```

### OpciÃ³n 2: Control Paso a Paso
```python
from modules import TedTalkAnalyzer

analyzer = TedTalkAnalyzer()
analyzer.setup_environment()        # Configurar ambiente
analyzer.load_data('ted_talks_en.csv')  # Cargar datos
analyzer.clean_data()               # Limpiar datos
analyzer.process_nlp_features()     # Procesar NLP
analyzer.create_visualizations()    # Crear grÃ¡ficos
analyzer.train_models()             # Entrenar modelos ML
```

### OpciÃ³n 3: Funciones EspecÃ­ficas
```python
# Importar solo lo que necesitas
from modules.data_cleaner import clean_dataset_professional
from modules.nlp_processor import analyze_sentiment
from modules.visualizer import create_wordcloud
from modules.ml_models import TedTalkClassifier

# Usar funciones individualmente
df_clean, log = clean_dataset_professional(df)
sentiment = analyze_sentiment("Amazing talk!")
create_wordcloud(text_data, "Mi Nube de Palabras")
```

## ğŸ“š MÃ³dulos Disponibles

### ğŸ”§ `environment_setup.py`
Configura el ambiente y descarga dependencias necesarias.

**Funciones principales:**
- `setup_environment()` - Instala todas las dependencias
- `download_transformer_models()` - Descarga modelos de Hugging Face
- `check_device()` - Verifica disponibilidad de GPU
- `get_environment_info()` - InformaciÃ³n del ambiente configurado

### ğŸ§¹ `data_cleaner.py`
Limpieza profesional de datos siguiendo estÃ¡ndares de la industria.

**Funciones principales:**
- `clean_dataset_professional(df)` - Pipeline completo de limpieza
- `remove_outliers_iqr(df, column)` - Elimina outliers usando IQR
- `create_popularity_categories(df)` - Crea categorÃ­as de popularidad
- `calculate_data_quality(df)` - Calcula puntuaciÃ³n de calidad
- `validate_data_quality(df)` - Valida calidad de datos

### ğŸ”¤ `nlp_processor.py`
Procesamiento avanzado de lenguaje natural.

**Funciones principales:**
- `process_text_features(df)` - Procesa todas las caracterÃ­sticas de texto
- `analyze_sentiment(text)` - AnÃ¡lisis de sentimientos con TextBlob
- `extract_named_entities(text, nlp_model)` - Extrae entidades con spaCy
- `extract_text_features(text)` - CaracterÃ­sticas bÃ¡sicas de texto
- `create_word_frequency_analysis(df)` - AnÃ¡lisis de frecuencia de palabras

### ğŸ“Š `visualizer.py`
Visualizaciones profesionales y grÃ¡ficos interactivos.

**Funciones principales:**
- `create_data_overview_plots(df)` - GrÃ¡ficos de resumen del dataset
- `create_correlation_heatmap(df)` - Matriz de correlaciÃ³n
- `create_sentiment_analysis_plots(df)` - GrÃ¡ficos de sentimientos
- `create_text_features_plots(df)` - GrÃ¡ficos de caracterÃ­sticas textuales
- `create_wordcloud(text_data)` - Nube de palabras
- `create_interactive_plots(df)` - GrÃ¡ficos interactivos con Plotly

### ğŸ¤– `ml_models.py`
Modelos de machine learning y evaluaciÃ³n.

**Clases y funciones principales:**
- `TedTalkClassifier` - Clase principal para clasificaciÃ³n
- `create_ml_pipeline(df)` - Pipeline completo de ML
- Modelos incluidos: Random Forest, Gradient Boosting, Logistic Regression, SVM
- EvaluaciÃ³n automÃ¡tica con mÃ©tricas: Accuracy, Precision, Recall, F1-Score, AUC

### ğŸ¯ `__init__.py`
MÃ³dulo principal que orquesta todo el anÃ¡lisis.

**Clase principal:**
- `TedTalkAnalyzer` - Orquesta todo el pipeline de anÃ¡lisis
- `quick_start()` - FunciÃ³n de inicio rÃ¡pido

## ğŸ”¥ CaracterÃ­sticas Principales

### âœ¨ Funcionalidades de NLP
- **AnÃ¡lisis de Sentimientos**: Polaridad y subjetividad con TextBlob
- **Entidades Nombradas**: ExtracciÃ³n con spaCy (personas, organizaciones, ubicaciones)
- **CaracterÃ­sticas Textuales**: Longitud, diversidad lÃ©xica, complejidad
- **TF-IDF**: VectorizaciÃ³n para modelos de ML
- **Frecuencia de Palabras**: AnÃ¡lisis de tÃ©rminos mÃ¡s comunes

### ğŸ“ˆ Modelos de Machine Learning
- **Random Forest**: Con optimizaciÃ³n de hiperparÃ¡metros
- **Gradient Boosting**: Para mejores predicciones
- **Logistic Regression**: Modelo base interpretable
- **SVM**: Para clasificaciÃ³n no lineal
- **EvaluaciÃ³n Completa**: ValidaciÃ³n cruzada, matrices de confusiÃ³n, mÃ©tricas

### ğŸ¨ Visualizaciones
- **GrÃ¡ficos EstÃ¡ticos**: matplotlib y seaborn
- **GrÃ¡ficos Interactivos**: Plotly para exploraciÃ³n
- **Nubes de Palabras**: WordCloud personalizable
- **Matrices de CorrelaciÃ³n**: AnÃ¡lisis de relaciones
- **Distribuciones**: Histogramas y box plots

### ğŸ›¡ï¸ Calidad de Datos
- **Limpieza Profesional**: EliminaciÃ³n de outliers con IQR
- **ValidaciÃ³n**: PuntuaciÃ³n automÃ¡tica de calidad
- **CategorizaciÃ³n**: ClasificaciÃ³n automÃ¡tica de popularidad
- **NormalizaciÃ³n**: Escalado de caracterÃ­sticas

## ğŸ“‹ Requisitos

### LibrerÃ­as Principales
```
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
plotly>=5.0.0
```

### LibrerÃ­as de NLP
```
nltk>=3.7
spacy>=3.4.0
transformers>=4.20.0
torch>=1.12.0
textblob>=0.17.0
wordcloud>=1.8.0
```

### InstalaciÃ³n AutomÃ¡tica
```python
from modules.environment_setup import setup_environment
setup_environment()  # Instala todo automÃ¡ticamente
```

## ğŸ¯ Ejemplos de Uso

### AnÃ¡lisis de Sentimientos Individual
```python
from modules.nlp_processor import analyze_sentiment

text = "This is an amazing and inspiring talk!"
result = analyze_sentiment(text)
print(f"Polaridad: {result['polarity']}")
print(f"Sentimiento: {result['sentiment_label']}")
```

### Crear VisualizaciÃ³n EspecÃ­fica
```python
from modules.visualizer import create_wordcloud
import pandas as pd

df = pd.read_csv('ted_talks_en.csv')
create_wordcloud(df['transcript'], "Palabras mÃ¡s Comunes")
```

### Entrenar Solo un Modelo EspecÃ­fico
```python
from modules.ml_models import TedTalkClassifier

classifier = TedTalkClassifier()
X, y = classifier.prepare_features(df)
X_train, X_test, y_train, y_test = classifier.split_data(X, y)
classifier.train_models(X_train, y_train)
results = classifier.evaluate_models(X_test, y_test)
```

## ğŸ” AnÃ¡lisis Incluidos

### ğŸ“Š ExploraciÃ³n de Datos
- DistribuciÃ³n de views y categorÃ­as de popularidad
- AnÃ¡lisis de longitud de transcripciones y tÃ­tulos
- Correlaciones entre variables numÃ©ricas
- DetecciÃ³n y eliminaciÃ³n de outliers

### ğŸ”¤ Procesamiento de Texto
- Limpieza y normalizaciÃ³n de texto
- ExtracciÃ³n de caracterÃ­sticas textuales
- AnÃ¡lisis de sentimientos (polaridad y subjetividad)
- IdentificaciÃ³n de entidades nombradas
- AnÃ¡lisis de frecuencia de palabras

### ğŸ¤– Machine Learning
- ClasificaciÃ³n de popularidad en 5 categorÃ­as
- Entrenamiento de mÃºltiples modelos
- EvaluaciÃ³n con mÃ©tricas estÃ¡ndar
- SelecciÃ³n automÃ¡tica del mejor modelo
- Importancia de caracterÃ­sticas

### ğŸ“ˆ Visualizaciones
- GrÃ¡ficos de distribuciÃ³n y correlaciÃ³n
- AnÃ¡lisis de sentimientos por categorÃ­a
- Matrices de confusiÃ³n de modelos
- GrÃ¡ficos interactivos para exploraciÃ³n
- Nubes de palabras temÃ¡ticas

## ğŸ¯ Objetivos del Proyecto

1. **Modularidad**: CÃ³digo organizado en mÃ³dulos reutilizables
2. **Flexibilidad**: Uso individual de funciones o pipeline completo
3. **Profesionalidad**: Siguiendo estÃ¡ndares de la industria
4. **Rendimiento**: Optimizado para datasets grandes
5. **Interpretabilidad**: Resultados claros y visualizaciones informativas

## ğŸ“ Notas Importantes

- **GPU Opcional**: El proyecto funciona en CPU, GPU mejora rendimiento
- **Memoria**: Recomendado 8GB+ RAM para datasets grandes
- **Tiempo**: El anÃ¡lisis completo puede tomar 10-30 minutos
- **Modularidad**: Cada mÃ³dulo puede usarse independientemente
- **Extensibilidad**: FÃ¡cil aÃ±adir nuevos modelos o caracterÃ­sticas

## ğŸ¤ Contribuciones

Este proyecto estÃ¡ diseÃ±ado para ser extensible. Puedes:
- AÃ±adir nuevos modelos de ML en `ml_models.py`
- Crear nuevas visualizaciones en `visualizer.py`
- Implementar tÃ©cnicas de NLP en `nlp_processor.py`
- Mejorar la limpieza de datos en `data_cleaner.py`

---

Â¡Esperamos que esta arquitectura modular te ayude a realizar anÃ¡lisis mÃ¡s eficientes y mantenibles! ğŸš€
